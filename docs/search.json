[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n  \n\n\n\n\nAn Image-to-Image Implementation\n\n\n\n\nDemonstation of an image-to-image implementation of the Stable Diffusion model.\n\n\n\n\n\n\nTuesday, 03 October 2023\n\n\n1 min\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "apps.html",
    "href": "apps.html",
    "title": "apps playground",
    "section": "",
    "text": "No matching items\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello üëã",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n\nforblog\nClick here to check out the latest Blogs.\n\n\n\n\n  \n\n\n\n\nAn Image-to-Image Implementation\n\n\n\n\nDemonstation of an image-to-image implementation of the Stable Diffusion model.\n\n\n\n\n\n\nOct 3, 2023\n\n\n\n\n\n\nNo matching items\n\n\n\n\nplayground\nClick here to play more in the playground.\n\n\n\n\nNo matching items\n\n\n\n\n\nContact Information"
  },
  {
    "objectID": "posts/00_Image-to-Image.html",
    "href": "posts/00_Image-to-Image.html",
    "title": "An Image-to-Image Implementation",
    "section": "",
    "text": "This Python code demonstrates the implementation of the Image-to-Image technique, allowing you to generate new images from existing ones with the help of textual prompts.\nExplore how this innovative approach combines images and text to create visually compelling artworks. Dive into the code to understand the mechanics behind this cutting-edge image generation technique.\n\n\nPip install necessary libraries (click to show/hide)\n!pip install -Uq diffusers transformers fastcore fastdownload\n\n\n\n\nImporting utilities (click to show/hide)\nfrom transformers import CLIPTextModel, CLIPTokenizer\nimport torch\nfrom diffusers import LMSDiscreteScheduler\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nfrom diffusers import AutoencoderKL, UNet2DConditionModel\nimport logging\nfrom fastdownload import FastDownload\nfrom pathlib import Path\nfrom huggingface_hub import notebook_login\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms\n\nif not (Path.home()/'.cache/huggingface'/'token').exists(): notebook_login()\nlogging.disable(logging.WARNING)\n\n\nWe need to load in the required libraries and set up the models.\n\ntokenizer = CLIPTokenizer.from_pretrained(\n    \"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\n\ntext_encoder = CLIPTextModel.from_pretrained(\n    \"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")\n\n# Here we use a different VAE to the original release, which has been fine-tuned for more steps\nvae = AutoencoderKL.from_pretrained(\n    \"stabilityai/sd-vae-ft-ema\", torch_dtype=torch.float16).to(\"cuda\")\n\nunet = UNet2DConditionModel.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")\n\nDefine the parameters.\n\nheight = 512\nwidth = 512\nnum_inference_steps =  70\nguidance_scale = 7.5\nbatch_size = 1\nbeta_start,beta_end = 0.00085,0.012\n\n\nscheduler = LMSDiscreteScheduler(beta_start=beta_start, beta_end=beta_end, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n\n\nplt.plot(scheduler.sigmas)\nplt.title('Noise Schedule')\nplt.xlabel('Sampling step')\nplt.ylabel('sigma')\nplt.show()\n\n\n\n\n\ndef prep_img(img_link : str) -&gt; torch.Tensor:\n    \"\"\"\n    Preprocesses an image from a given link.\n\n    Args:\n        img_link (str): The URL or path to the image file.\n\n    Returns:\n        torch.Tensor: A tensor representing the preprocessed image.\n    \"\"\"\n    \n    p = FastDownload().download(img_link)\n    init_image = Image.open(p).convert(\"RGB\").resize((512, 512))\n    return transforms.ToTensor()(init_image)\n\nThe image we will use as a starting point.\n\n\nDownloading the image (click to show/hide)\nlink = \"https://cdn-uploads.huggingface.co/production/uploads/1664665907257-noauth.png\"\ntransformed_image = prep_img(link)\n# show image in notebook.\np = FastDownload().download(link)\ndisplay(Image.open(p).convert(\"RGB\").resize((512, 512)))\n\n\n\n\n\n\ndef tokenization(prompt: list, max_len : int = None) - &gt; torch.Tensor:\n    \"\"\"\n    Tokenizes a text prompt and returns the corresponding encoded tensor.\n\n    Args:\n        prompt (list): The input text prompt to be tokenized.\n        max_len (int, optional): The maximum length of the tokenized sequence. If not specified,\n            it defaults to the maximum length allowed by the tokenizer.\n\n    Returns:\n        torch.Tensor: A tensor containing the encoded representation of the tokenized prompt.\n    \"\"\"\n\n    if max_len is None : max_len = tokenizer.model_max_length\n    \n    tokenized_prompt = tokenizer(prompt, padding=\"max_length\", max_length = max_len, truncation=True, return_tensors='pt')\n    return text_encoder(tokenized_prompt.input_ids.to('cuda'))[0].half()\n\ndef make_image(latent: torch.Tensor):\n    \"\"\"\n    Converts a tensor representation of an image into a PIL Image.\n\n    Args:\n        latent (torch.Tensor): A tensor representing an image.\n\n    Returns:\n        PIL.Image.Image: A PIL Image representing the image.\n    \"\"\"\n    image = (latent/2+0.5).clamp(0,1).detach().cpu().permute(1, 2, 0).numpy()\n    return Image.fromarray((image*255).round().astype(\"uint8\"))\n\n\n\nTo ensure the effectiveness of this solution, it is essential to incorporate the ‚Äústart_step‚Äù parameter.\nEssentially, we aim to prevent excessive noise from being added to the input image, particularly avoiding the most intense noise additions.\nAfter this initial step, we can proceed with the looping process.\n\nIn summary, the key to success here is to introduce the ‚Äústart_step‚Äù parameter, which helps us avoid excessive noise in the early stages and then continue with the loop as intended.\n\n\ndef create_sample(prompt: list,transformed_image: torch.Tensor ,guidance_scale: float = 7.5, seed: int = 5, steps: int = 70,start_step: int = 10):\n    '''\n    Generate a sample image based on a text prompt, provided image and guidance parameters.\n\n    Args:\n        prompt (list): A list of text prompts.\n        transformed_image (torch.Tensor): A tensor representing the transformed image.\n        guidance_scale (float, optional): The scale factor for guiding the generation process.\n        seed (int, optional): Seed for random number generation. Default is 5.\n        steps (int, optional): The total number of steps for the generation process. Default is 70.\n        start_step (int, optional): The step at which the generation process starts. Default is 10.\n\n    Returns:\n        torch.Tensor: A tensor representing the generated sample.\n\n    This function generates an image based on the provided text prompts , transformed image and parametrs.It uses a predefined\n    VAE model to encode the image and then applies noise and guidance to generate the sample.It iteratively\n    refines the image by adding noise and updating the latent representation. The guidance scale controls the\n    influence of the text prompts on the image. The generated image is returned as a PyTorch tensor.\n\n    Example:\n    &gt;&gt;&gt; prompt = [\"Translate the following English sentence to French: 'Hello, how are you?'\"]\n    &gt;&gt;&gt; transformed_image = prep_img(image_link)\n    &gt;&gt;&gt; generated_sample = create_sample(prompt, transformed_image)\n    '''\n    \n    bs = 1  # Implementation for only a single prompt.\n    text = tokenization(prompt)\n    uncond = tokenization([\"\"] * bs, text.shape[1])\n    emb = torch.cat([uncond, text])\n    if seed:\n        torch.manual_seed(seed)\n\n    # Encode image\n    image_latent = vae.encode((transformed_image.unsqueeze(0).half().to('cuda'))).latent_dist.sample()\n    image_latent = vae.config.scaling_factor * image_latent\n\n    # Create noise\n    scheduler.set_timesteps(steps)\n    noise_latents = torch.randn_like(image_latent)\n\n    latents = scheduler.add_noise(image_latent, noise_latents,\n                                  timesteps=torch.tensor([scheduler.timesteps[start_step]]))\n\n    for i, ts in enumerate(tqdm(scheduler.timesteps)):\n        if i &gt;= start_step:  # Skip the batches of noise that don't affect the input image.\n            inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n            with torch.no_grad():\n                noise_pred_uncond, noise_pred_text = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n            pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n            latents = scheduler.step(pred, ts, latents).prev_sample\n\n    with torch.no_grad():\n        return vae.decode(1 / 0.18215 * latents).sample\n\n\nprompt =  ['Wolf howling at the moon, photorealistic 4K']\n#prompt = ['unicorn']\n#prompt = ['a kids drawing of bacteria, cartoon style']\n#prompt = [' Horse looking at the morning sun, photorealistic 4K']\n\n\nimage = create_sample(prompt,transformed_image,steps = 50,seed = 1000)\n\n\ndisplay(make_image(image[0]))"
  },
  {
    "objectID": "posts/00_Image-to-Image.html#denoising-loop",
    "href": "posts/00_Image-to-Image.html#denoising-loop",
    "title": "An Image-to-Image Implementation",
    "section": "",
    "text": "To ensure the effectiveness of this solution, it is essential to incorporate the ‚Äústart_step‚Äù parameter.\nEssentially, we aim to prevent excessive noise from being added to the input image, particularly avoiding the most intense noise additions.\nAfter this initial step, we can proceed with the looping process.\n\nIn summary, the key to success here is to introduce the ‚Äústart_step‚Äù parameter, which helps us avoid excessive noise in the early stages and then continue with the loop as intended.\n\n\ndef create_sample(prompt: list,transformed_image: torch.Tensor ,guidance_scale: float = 7.5, seed: int = 5, steps: int = 70,start_step: int = 10):\n    '''\n    Generate a sample image based on a text prompt, provided image and guidance parameters.\n\n    Args:\n        prompt (list): A list of text prompts.\n        transformed_image (torch.Tensor): A tensor representing the transformed image.\n        guidance_scale (float, optional): The scale factor for guiding the generation process.\n        seed (int, optional): Seed for random number generation. Default is 5.\n        steps (int, optional): The total number of steps for the generation process. Default is 70.\n        start_step (int, optional): The step at which the generation process starts. Default is 10.\n\n    Returns:\n        torch.Tensor: A tensor representing the generated sample.\n\n    This function generates an image based on the provided text prompts , transformed image and parametrs.It uses a predefined\n    VAE model to encode the image and then applies noise and guidance to generate the sample.It iteratively\n    refines the image by adding noise and updating the latent representation. The guidance scale controls the\n    influence of the text prompts on the image. The generated image is returned as a PyTorch tensor.\n\n    Example:\n    &gt;&gt;&gt; prompt = [\"Translate the following English sentence to French: 'Hello, how are you?'\"]\n    &gt;&gt;&gt; transformed_image = prep_img(image_link)\n    &gt;&gt;&gt; generated_sample = create_sample(prompt, transformed_image)\n    '''\n    \n    bs = 1  # Implementation for only a single prompt.\n    text = tokenization(prompt)\n    uncond = tokenization([\"\"] * bs, text.shape[1])\n    emb = torch.cat([uncond, text])\n    if seed:\n        torch.manual_seed(seed)\n\n    # Encode image\n    image_latent = vae.encode((transformed_image.unsqueeze(0).half().to('cuda'))).latent_dist.sample()\n    image_latent = vae.config.scaling_factor * image_latent\n\n    # Create noise\n    scheduler.set_timesteps(steps)\n    noise_latents = torch.randn_like(image_latent)\n\n    latents = scheduler.add_noise(image_latent, noise_latents,\n                                  timesteps=torch.tensor([scheduler.timesteps[start_step]]))\n\n    for i, ts in enumerate(tqdm(scheduler.timesteps)):\n        if i &gt;= start_step:  # Skip the batches of noise that don't affect the input image.\n            inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n            with torch.no_grad():\n                noise_pred_uncond, noise_pred_text = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n            pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n            latents = scheduler.step(pred, ts, latents).prev_sample\n\n    with torch.no_grad():\n        return vae.decode(1 / 0.18215 * latents).sample\n\n\nprompt =  ['Wolf howling at the moon, photorealistic 4K']\n#prompt = ['unicorn']\n#prompt = ['a kids drawing of bacteria, cartoon style']\n#prompt = [' Horse looking at the morning sun, photorealistic 4K']\n\n\nimage = create_sample(prompt,transformed_image,steps = 50,seed = 1000)\n\n\ndisplay(make_image(image[0]))"
  }
]