[
  {
    "objectID": "posts/00_lr_scheduler_from_scratch.html",
    "href": "posts/00_lr_scheduler_from_scratch.html",
    "title": "LR Schedulers Implementation From Scratch",
    "section": "",
    "text": "Importing utilities (click to show/hide)\nimport torch,math,functools\nimport matplotlib.pyplot as plt\nfrom functools import partial\nimport pdb\nfrom tinyai.datasets import *\nfrom tinyai.conv import *\nfrom tinyai.learner import *\nfrom tinyai.activations import *\nfrom tinyai.init import *\nfrom tinyai.sgd import *\nfrom datasets import load_dataset\nimport torchvision.transforms.functional as TF,torch.nn.functional as F\nfrom torch import tensor,nn,optim\nimport fastcore.all as fc\nfrom torch.optim import lr_scheduler\nfrom torcheval.metrics import MulticlassAccuracy\nx = torch.linspace(0,10,10)\nlr = 5\nprint(x,math.pi)\n\ntensor([ 0.0000,  1.1111,  2.2222,  3.3333,  4.4444,  5.5556,  6.6667,  7.7778,\n         8.8889, 10.0000]) 3.141592653589793\nHow we want our learning rate to look at.\ndef plot_thing(f,lr,steps):\n    x= torch.linspace(0,math.pi,steps)\n    plt.plot(x,(f(x) +1)/2 * lr)\nplot_thing(partial(torch.cos),lr,steps=100)\n\n\n\n\nFigure: Plot of Cosine Function"
  },
  {
    "objectID": "posts/00_lr_scheduler_from_scratch.html#preparing-the-learner-for-training.",
    "href": "posts/00_lr_scheduler_from_scratch.html#preparing-the-learner-for-training.",
    "title": "LR Schedulers Implementation From Scratch",
    "section": "Preparing the learner for training.",
    "text": "Preparing the learner for training.\n\n\nCode for learner. (click to show/hide)\nact_gr = partial(GeneralRelu, leak=0.1, sub=0.4)\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\nastats = ActivationStats(fc.risinstance(GeneralRelu))\ncbs = [DeviceCB(), metrics, ProgressCB(plot=True), astats]\niw = partial(init_weights, leaky=0.1)\nset_seed(42)\n\nlr,epochs = 1e-2,5\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\ntmax = epochs * len(dls.train)\nsched = partial(CosAnnLR,tmax)\n#sched = partial(lr_scheduler.CosineAnnealingLR,T_max = tmax) \n# Testing if it works with pytorch's CosineAnnealingLR \n\nrecord = RecorderCB(lr=_lr)\nxtra = [BatchSchedCB(sched),record]\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\n\n\n\n\nCode\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.806\n0.529\n0\ntrain\n\n\n0.853\n0.404\n0\neval\n\n\n0.876\n0.338\n1\ntrain\n\n\n0.872\n0.349\n1\neval\n\n\n0.892\n0.295\n2\ntrain\n\n\n0.882\n0.326\n2\neval\n\n\n0.904\n0.264\n3\ntrain\n\n\n0.887\n0.316\n3\neval\n\n\n0.910\n0.248\n4\ntrain\n\n\n0.887\n0.310\n4\neval\n\n\n\n\n\n\n\n\n\n\nCode\nrecord.plot()\n\n\n\n\n\nFigure: Plot of learning rate throughout the learning process.\n\n\n\n\n\nastats.color_dim()\n\n\n\n\nFigure: Plot of Weight‚Äôs distribution.\n\n\n\n\n\nastats.plot_stats()\n\n\n\n\nFigure: Plot of Weight‚Äôs Means and Stdves throughout the learning process.\n\n\n\n\n\nastats.dead_chart()\n\n\n\n\nFigure: Plot of Weight‚Äôs that are = 0."
  },
  {
    "objectID": "posts/00_lr_scheduler_from_scratch.html#preparing-the-learner-for-training.-1",
    "href": "posts/00_lr_scheduler_from_scratch.html#preparing-the-learner-for-training.-1",
    "title": "LR Schedulers Implementation From Scratch",
    "section": "Preparing the learner for training.",
    "text": "Preparing the learner for training.\n\n\nCode for learner. (click to show/hide)\nact_gr = partial(GeneralRelu, leak=0.1, sub=0.4)\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\nastats = ActivationStats(fc.risinstance(GeneralRelu))\ncbs = [DeviceCB(), metrics, ProgressCB(plot=True), astats]\niw = partial(init_weights, leaky=0.1)\n\nset_seed(42)\n\nlr,epochs = 1e-2,5\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\ntmax = epochs * len(dls.train)\nsched = partial(OneCycleLR,tmax)\n#sched = partial(lr_scheduler.OneCycleLR,max_lr = lr,total_steps = tmax) # Testing if it works with pytorch's CosineAnnealingLR \n\nrecord = RecorderCB(lr=_lr, mom=_beta1)\nxtra = [BatchSchedCB(sched),record]\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\n\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.723\n0.827\n0\ntrain\n\n\n0.822\n0.485\n0\neval\n\n\n0.860\n0.386\n1\ntrain\n\n\n0.864\n0.368\n1\neval\n\n\n0.887\n0.310\n2\ntrain\n\n\n0.877\n0.338\n2\neval\n\n\n0.902\n0.268\n3\ntrain\n\n\n0.882\n0.316\n3\neval\n\n\n0.912\n0.242\n4\ntrain\n\n\n0.888\n0.303\n4\neval\n\n\n\n\n\n\n\n\n\nNote: If you happened to know why does the learning doesn‚Äôt go smoothly at the beginning, u can dm me on discord @afterhoursbilly\n\n\n\nCode\nrecord.plot()\n\n\n\n\n\nFigure: Plot of Learning Rate and Momentum throughout the learning process.\n\n\n\n\n\n\n\n\nastats.plot_stats()\n\n\n\n\nFigure: Plot of Weight‚Äôs Means and Stdves throughout the learning process.\n\n\n\n\n\nastats.dead_chart()\n\n\n\n\nFigure: Plot of Weight‚Äôs that are = to 0.\n\n\n\n\n\nastats.color_dim()\n\n\n\n\nFigure: Plot of Weight‚Äôs distribution.\n\n\n\n\n\nOneCycle Summary\n\nInspired by paper, & fast.ai 22part course\n\n\nThis CLR implements minmum and maximum learning rate boundaries\n\nWe could also add a phase where learning rate is at its maximum for 5-10% of the training.\n\nclass OneCycleLR:\n    '''\n    Modified version after looking up papers.\n    '''\n    def __init__(self, tmax, optim, warm_up:float = 0.30):\n        self.optim = optim\n        self.initial_lr,self.min_lr = self.optim.param_groups[0]['lr'],self.optim.param_groups[0]['lr']//20\n        self.beta,self.beta_2 = self.optim.param_groups[0]['betas']\n        self.max_beta, self.min_beta = self.beta + 0.05, self.beta - 0.05\n        self.warm_up = warm_up\n        self.warm_up_steps = int(tmax * self.warm_up)\n        self.annealing_steps = tmax - self.warm_up_steps\n        self.cur_step = 0\n\n    def cosine_annealing(self,phase,min,max):\n        return min + (max-min) * ((math.cos(math.pi * phase)+1)/2)\n    \n    def step(self):\n        \n        # warm_up phase\n        if self.cur_step &lt;= self.warm_up_steps:\n            # Increasing learning rate\n            phase = self.cur_step / self.warm_up_steps\n            adjusted_lr = self.cosine_annealing(phase,self.initial_lr,self.min_lr)\n            adjusted_beta = self.cosine_annealing(phase,self.min_beta,self.max_beta)\n        else:\n            # Decreasing learning rate\n            phase = (self.cur_step - self.warm_up_steps) / self.annealing_steps\n            adjusted_lr = self.cosine_annealing(phase,self.min_lr,self.initial_lr)\n            adjusted_beta = self.cosine_annealing(phase,self.max_beta,self.min_beta)\n\n        # adjusted_lr min_max\n        self.optim.param_groups[0]['lr'] = adjusted_lr\n        self.optim.param_groups[0]['betas'] = (adjusted_beta,self.beta_2)\n        self.cur_step += 1\n\n\nlr,epochs = 1e-2,5\nmodel = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\ntmax = epochs * len(dls.train)\nsched = partial(OneCycleLR,tmax)\n\nrecord = RecorderCB(lr=_lr, mom=_beta1)\nxtra = [BatchSchedCB(sched),record]\nlearn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.696\n0.921\n0\ntrain\n\n\n0.825\n0.476\n0\neval\n\n\n0.857\n0.391\n1\ntrain\n\n\n0.861\n0.385\n1\neval\n\n\n0.884\n0.317\n2\ntrain\n\n\n0.875\n0.348\n2\neval\n\n\n0.900\n0.272\n3\ntrain\n\n\n0.882\n0.322\n3\neval\n\n\n0.913\n0.241\n4\ntrain\n\n\n0.886\n0.315\n4\neval"
  },
  {
    "objectID": "apps.html",
    "href": "apps.html",
    "title": "apps playground",
    "section": "",
    "text": "No matching items\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello üëã",
    "section": "",
    "text": "My name is Szymon and, I am an aspiring Machine/Deep Learning Engineer, currently in my third year of Computer Science. If you are already here, you can check out my blog posts and demo apps in the playground below. You can find most of the code for my projects on my GitHub.\n\n\nblog\nClick here to check out the all blog posts.\n\n\n\n\n  \n\n\n\n\nLR Schedulers Implementation From Scratch\n\n\n\n\nImplementation of cosine annealing and OneCycle learning rate schedulers from scratch using tinyai mini-framework\n\n\n\n\n\n\nNov 13, 2023\n\n\n\n\n\n\n  \n\n\n\n\nAn Image-to-Image Implementation\n\n\n\n\nDemonstation of an image-to-image implementation of the Stable Diffusion model.\n\n\n\n\n\n\nOct 3, 2023\n\n\n\n\n\n\nNo matching items\n\n\n\n\nplayground\nClick here to play more in the playground.\n\n\n\n\nNo matching items\n\n\n\n\n\nContact Information"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n  \n\n\n\n\nLR Schedulers Implementation From Scratch\n\n\n\n\nImplementation of cosine annealing and OneCycle learning rate schedulers from scratch using tinyai mini-framework\n\n\n\n\n\n\nMonday, 13 November 2023\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nAn Image-to-Image Implementation\n\n\n\n\nDemonstation of an image-to-image implementation of the Stable Diffusion model.\n\n\n\n\n\n\nTuesday, 03 October 2023\n\n\n1 min\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "posts/00_Image-to-Image.html",
    "href": "posts/00_Image-to-Image.html",
    "title": "An Image-to-Image Implementation",
    "section": "",
    "text": "This Python code demonstrates the implementation of the Image-to-Image technique, allowing you to generate new images from existing ones with the help of textual prompts.\nExplore how this innovative approach combines images and text to create visually compelling artworks. Dive into the code to understand the mechanics behind this cutting-edge image generation technique.\n\n\nPip install necessary libraries (click to show/hide)\n!pip install -Uq diffusers transformers fastcore fastdownload\n\n\n\n\nImporting utilities (click to show/hide)\nfrom transformers import CLIPTextModel, CLIPTokenizer\nimport torch\nfrom diffusers import LMSDiscreteScheduler\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nfrom diffusers import AutoencoderKL, UNet2DConditionModel\nimport logging\nfrom fastdownload import FastDownload\nfrom pathlib import Path\nfrom huggingface_hub import notebook_login\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms\n\nif not (Path.home()/'.cache/huggingface'/'token').exists(): notebook_login()\nlogging.disable(logging.WARNING)\n\n\nWe need to load in the required libraries and set up the models.\n\ntokenizer = CLIPTokenizer.from_pretrained(\n    \"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\n\ntext_encoder = CLIPTextModel.from_pretrained(\n    \"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")\n\n# Here we use a different VAE to the original release, which has been fine-tuned for more steps\nvae = AutoencoderKL.from_pretrained(\n    \"stabilityai/sd-vae-ft-ema\", torch_dtype=torch.float16).to(\"cuda\")\n\nunet = UNet2DConditionModel.from_pretrained(\n    \"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")\n\nDefine the parameters.\n\nheight = 512\nwidth = 512\nnum_inference_steps =  70\nguidance_scale = 7.5\nbatch_size = 1\nbeta_start,beta_end = 0.00085,0.012\n\n\nscheduler = LMSDiscreteScheduler(beta_start=beta_start, beta_end=beta_end, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n\n\nplt.plot(scheduler.sigmas)\nplt.title('Noise Schedule')\nplt.xlabel('Sampling step')\nplt.ylabel('sigma')\nplt.show()\n\n\n\n\n\ndef prep_img(img_link : str) -&gt; torch.Tensor:\n    \"\"\"\n    Preprocesses an image from a given link.\n\n    Args:\n        img_link (str): The URL or path to the image file.\n\n    Returns:\n        torch.Tensor: A tensor representing the preprocessed image.\n    \"\"\"\n    \n    p = FastDownload().download(img_link)\n    init_image = Image.open(p).convert(\"RGB\").resize((512, 512))\n    return transforms.ToTensor()(init_image)\n\nThe image we will use as a starting point.\n\n\nDownloading the image (click to show/hide)\nlink = \"https://cdn-uploads.huggingface.co/production/uploads/1664665907257-noauth.png\"\ntransformed_image = prep_img(link)\n# show image in notebook.\np = FastDownload().download(link)\ndisplay(Image.open(p).convert(\"RGB\").resize((512, 512)))\n\n\n\n\n\n\ndef tokenization(prompt: list, max_len : int = None) - &gt; torch.Tensor:\n    \"\"\"\n    Tokenizes a text prompt and returns the corresponding encoded tensor.\n\n    Args:\n        prompt (list): The input text prompt to be tokenized.\n        max_len (int, optional): The maximum length of the tokenized sequence. If not specified,\n            it defaults to the maximum length allowed by the tokenizer.\n\n    Returns:\n        torch.Tensor: A tensor containing the encoded representation of the tokenized prompt.\n    \"\"\"\n\n    if max_len is None : max_len = tokenizer.model_max_length\n    \n    tokenized_prompt = tokenizer(prompt, padding=\"max_length\", max_length = max_len, truncation=True, return_tensors='pt')\n    return text_encoder(tokenized_prompt.input_ids.to('cuda'))[0].half()\n\ndef make_image(latent: torch.Tensor):\n    \"\"\"\n    Converts a tensor representation of an image into a PIL Image.\n\n    Args:\n        latent (torch.Tensor): A tensor representing an image.\n\n    Returns:\n        PIL.Image.Image: A PIL Image representing the image.\n    \"\"\"\n    image = (latent/2+0.5).clamp(0,1).detach().cpu().permute(1, 2, 0).numpy()\n    return Image.fromarray((image*255).round().astype(\"uint8\"))\n\n\n\nTo ensure the effectiveness of this solution, it is essential to incorporate the ‚Äústart_step‚Äù parameter.\nEssentially, we aim to prevent excessive noise from being added to the input image, particularly avoiding the most intense noise additions.\nAfter this initial step, we can proceed with the looping process.\n\nIn summary, the key to success here is to introduce the ‚Äústart_step‚Äù parameter, which helps us avoid excessive noise in the early stages and then continue with the loop as intended.\n\n\ndef create_sample(prompt: list,transformed_image: torch.Tensor ,guidance_scale: float = 7.5, seed: int = 5, steps: int = 70,start_step: int = 10):\n    '''\n    Generate a sample image based on a text prompt, provided image and guidance parameters.\n\n    Args:\n        prompt (list): A list of text prompts.\n        transformed_image (torch.Tensor): A tensor representing the transformed image.\n        guidance_scale (float, optional): The scale factor for guiding the generation process.\n        seed (int, optional): Seed for random number generation. Default is 5.\n        steps (int, optional): The total number of steps for the generation process. Default is 70.\n        start_step (int, optional): The step at which the generation process starts. Default is 10.\n\n    Returns:\n        torch.Tensor: A tensor representing the generated sample.\n\n    This function generates an image based on the provided text prompts , transformed image and parametrs.It uses a predefined\n    VAE model to encode the image and then applies noise and guidance to generate the sample.It iteratively\n    refines the image by adding noise and updating the latent representation. The guidance scale controls the\n    influence of the text prompts on the image. The generated image is returned as a PyTorch tensor.\n\n    Example:\n    &gt;&gt;&gt; prompt = [\"Translate the following English sentence to French: 'Hello, how are you?'\"]\n    &gt;&gt;&gt; transformed_image = prep_img(image_link)\n    &gt;&gt;&gt; generated_sample = create_sample(prompt, transformed_image)\n    '''\n    \n    bs = 1  # Implementation for only a single prompt.\n    text = tokenization(prompt)\n    uncond = tokenization([\"\"] * bs, text.shape[1])\n    emb = torch.cat([uncond, text])\n    if seed:\n        torch.manual_seed(seed)\n\n    # Encode image\n    image_latent = vae.encode((transformed_image.unsqueeze(0).half().to('cuda'))).latent_dist.sample()\n    image_latent = vae.config.scaling_factor * image_latent\n\n    # Create noise\n    scheduler.set_timesteps(steps)\n    noise_latents = torch.randn_like(image_latent)\n\n    latents = scheduler.add_noise(image_latent, noise_latents,\n                                  timesteps=torch.tensor([scheduler.timesteps[start_step]]))\n\n    for i, ts in enumerate(tqdm(scheduler.timesteps)):\n        if i &gt;= start_step:  # Skip the batches of noise that don't affect the input image.\n            inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n            with torch.no_grad():\n                noise_pred_uncond, noise_pred_text = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n            pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n            latents = scheduler.step(pred, ts, latents).prev_sample\n\n    with torch.no_grad():\n        return vae.decode(1 / 0.18215 * latents).sample\n\n\nprompt =  ['Wolf howling at the moon, photorealistic 4K']\n#prompt = ['unicorn']\n#prompt = ['a kids drawing of bacteria, cartoon style']\n#prompt = [' Horse looking at the morning sun, photorealistic 4K']\n\n\nimage = create_sample(prompt,transformed_image,steps = 50,seed = 1000)\n\n\ndisplay(make_image(image[0]))"
  },
  {
    "objectID": "posts/00_Image-to-Image.html#denoising-loop",
    "href": "posts/00_Image-to-Image.html#denoising-loop",
    "title": "An Image-to-Image Implementation",
    "section": "",
    "text": "To ensure the effectiveness of this solution, it is essential to incorporate the ‚Äústart_step‚Äù parameter.\nEssentially, we aim to prevent excessive noise from being added to the input image, particularly avoiding the most intense noise additions.\nAfter this initial step, we can proceed with the looping process.\n\nIn summary, the key to success here is to introduce the ‚Äústart_step‚Äù parameter, which helps us avoid excessive noise in the early stages and then continue with the loop as intended.\n\n\ndef create_sample(prompt: list,transformed_image: torch.Tensor ,guidance_scale: float = 7.5, seed: int = 5, steps: int = 70,start_step: int = 10):\n    '''\n    Generate a sample image based on a text prompt, provided image and guidance parameters.\n\n    Args:\n        prompt (list): A list of text prompts.\n        transformed_image (torch.Tensor): A tensor representing the transformed image.\n        guidance_scale (float, optional): The scale factor for guiding the generation process.\n        seed (int, optional): Seed for random number generation. Default is 5.\n        steps (int, optional): The total number of steps for the generation process. Default is 70.\n        start_step (int, optional): The step at which the generation process starts. Default is 10.\n\n    Returns:\n        torch.Tensor: A tensor representing the generated sample.\n\n    This function generates an image based on the provided text prompts , transformed image and parametrs.It uses a predefined\n    VAE model to encode the image and then applies noise and guidance to generate the sample.It iteratively\n    refines the image by adding noise and updating the latent representation. The guidance scale controls the\n    influence of the text prompts on the image. The generated image is returned as a PyTorch tensor.\n\n    Example:\n    &gt;&gt;&gt; prompt = [\"Translate the following English sentence to French: 'Hello, how are you?'\"]\n    &gt;&gt;&gt; transformed_image = prep_img(image_link)\n    &gt;&gt;&gt; generated_sample = create_sample(prompt, transformed_image)\n    '''\n    \n    bs = 1  # Implementation for only a single prompt.\n    text = tokenization(prompt)\n    uncond = tokenization([\"\"] * bs, text.shape[1])\n    emb = torch.cat([uncond, text])\n    if seed:\n        torch.manual_seed(seed)\n\n    # Encode image\n    image_latent = vae.encode((transformed_image.unsqueeze(0).half().to('cuda'))).latent_dist.sample()\n    image_latent = vae.config.scaling_factor * image_latent\n\n    # Create noise\n    scheduler.set_timesteps(steps)\n    noise_latents = torch.randn_like(image_latent)\n\n    latents = scheduler.add_noise(image_latent, noise_latents,\n                                  timesteps=torch.tensor([scheduler.timesteps[start_step]]))\n\n    for i, ts in enumerate(tqdm(scheduler.timesteps)):\n        if i &gt;= start_step:  # Skip the batches of noise that don't affect the input image.\n            inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n            with torch.no_grad():\n                noise_pred_uncond, noise_pred_text = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n            pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n            latents = scheduler.step(pred, ts, latents).prev_sample\n\n    with torch.no_grad():\n        return vae.decode(1 / 0.18215 * latents).sample\n\n\nprompt =  ['Wolf howling at the moon, photorealistic 4K']\n#prompt = ['unicorn']\n#prompt = ['a kids drawing of bacteria, cartoon style']\n#prompt = [' Horse looking at the morning sun, photorealistic 4K']\n\n\nimage = create_sample(prompt,transformed_image,steps = 50,seed = 1000)\n\n\ndisplay(make_image(image[0]))"
  }
]